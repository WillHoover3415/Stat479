---
title: "STAT 479 project codes"
author: "Roy Son"
date: "2025-10-07"
output: html_document
---

### This part of the codes are just calling up the funcion calls so it loads the necessary packages I need for this project in R. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("hoopR")
library("dplyr")
library("tidyr")
library("ggplot2")
library(stringr)
library(tictoc)
library(devtools)
library(pacman)
```


### Shots_2025 dataset contains all shot observations from NBA players during the 2025 season. However,not all available features are relelvant to the modeling,and keeping unnecessary variables could cause confusion or noise later in the analysis. To prevent this, I created a new dataset called shots that keeps only the key variables used for modeling: player name, team name, shot type(2pt or 3pt), shot zone, points scored, and the shot flag indiciating whether the shot was made. Lastly team names were mapped to abbreviations to ensure consistency across datasets, and shot distances were converted to numeric values. Also, I encoded new variable Points_rule to classify each attempt as a 3 point make, 2 point make, or o point(missed shot). 


```{r}
shots = shots_2025 %>%
  select(PLAYER_NAME, TEAM_NAME, SHOT_DISTANCE, SHOT_TYPE, SHOT_ZONE_BASIC, PTS, SHOT_MADE_FLAG)

Team_abbreviation = c( "Atlanta Hawks" = "ATL", "Boston Celtics" = "BOS", "Brooklyn Nets" = "BKN",
  "Charlotte Hornets" = "CHA", "Chicago Bulls" = "CHI", "Cleveland Cavaliers" = "CLE",
  "Dallas Mavericks" = "DAL", "Denver Nuggets" = "DEN", "Detroit Pistons" = "DET",
  "Golden State Warriors" = "GSW", "Houston Rockets" = "HOU", "Indiana Pacers" = "IND",
  "LA Clippers" = "LAC", "Los Angeles Lakers" = "LAL", "Memphis Grizzlies" = "MEM",
  "Miami Heat" = "MIA", "Milwaukee Bucks" = "MIL", "Minnesota Timberwolves" = "MIN",
  "New Orleans Pelicans" = "NOP", "New York Knicks" = "NYK", "Oklahoma City Thunder" = "OKC",
  "Orlando Magic" = "ORL", "Philadelphia 76ers" = "PHI", "Phoenix Suns" = "PHX",
  "Portland Trail Blazers" = "POR", "Sacramento Kings" = "SAC", "San Antonio Spurs" = "SAS",
  "Toronto Raptors" = "TOR", "Utah Jazz" = "UTA", "Washington Wizards" = "WAS")

shots = shots %>%
  mutate(SHOT_DISTANCE = as.numeric(SHOT_DISTANCE),
         team_abbreviation = Team_abbreviation[TEAM_NAME],
         Points_rule = case_when(
           SHOT_MADE_FLAG == 1 & SHOT_TYPE == "3PT Field Goal" ~ 3,
           SHOT_MADE_FLAG == 1 & SHOT_TYPE == "2PT Field Goal" ~ 2,
           TRUE ~ 0
         ))

shots %>% 
  arrange(desc(SHOT_DISTANCE))
```
### Now I began constructing the Naive Binning and Averaging model, which estimates each shot's expected point value based on its distance and shot zone. I wanted to compare each team's actual scoring output against what they were expected to score,given league average efficiency for the same shot locations. First, I filtered out any observations missing shot distances and created a new variable, distance_ft, by floorig each distance to the nearest whole foot. This had to be made because I was not able to aggregate attempts with comparable difficutly if there was no converstion of continuous host distances into one foot bins. Next, I grouped all shots by distance_ft, SHOT_ZONE_BASIC, and SHOT_TYPE to calculate baseline performance metric for the entire league. Each combination of distance, zone, and type forms a bin that represents average shot value across the league. In league_average_2.0, the most important metric is expected_points_rule as it represnets how many points a average player would be expected to score per attempt from that spot on the floor. To estimate how each team should have performed, I merged these league average stats back into the shot level dataset. The left_join matches each individual shot to its corresponding league average expected points based on distance, zone, and type of shot. The new variable expected_goal assigns a model based expected value to every attempted shots of what the average player would have scored from that exact location. I then aggregated all shots at the team level to compare actual total and expected total. expected_points shows a sum of expected values from league averages, actual_points shows a actual points scored by the team, and residual shows the difference between actual performance and expected performance: positive = overperformed offensively, negative = underperformed offesnively. This allows a comparioson of offesnive execution against shot quality. The first graph, shows the bar plot raking NBA teams by their offesnive over or underperformance relative to the Native Binning model. Our model suggests that Teams with positive residuals(blue bars): Phoenix Suns, Denver Nuggets, and Sacramento Kings scored more than the model expected given their shots. This suggest these teams had elite shooting efficeincy. It also suggests that Teams with negative residuals(red bars): Toronto Raptors, Charlotte Hornets, and Portland Trail Blazers scored less than the model expected, this suggests that theset teams had poor shooting efficiency. The second graph showed each team's total expected points based on how many points the team would be expected to score it all shots were within league average efficiency. Teams like Atlanta Hawks, Memphis Grizzlies, and Houston Rockets produced the highest expected totals, indicating they made a lot of shots at efficient locations while teams like Charlotte Hornets and Philadelphhia 76ers made a lot of shots at inefficient locations. These plots provide a comprehensive view of offensive performance: shooting efficiency and shot selection. 

```{r}
shots_by_ft = shots %>%
  filter(!is.na(SHOT_DISTANCE)) %>%
  mutate(distance_ft = floor(SHOT_DISTANCE))



league_avearage_2.0 = shots_by_ft %>%
  group_by(distance_ft, SHOT_ZONE_BASIC, SHOT_TYPE)%>%
  summarize(
    attempt_league = n(),
    made_shots_league = sum(SHOT_MADE_FLAG == 1, na.rm = TRUE),
    fg_percentage_league = attempt_league / made_shots_league, 
    expected_points_league = mean(Points_rule, na.rm = TRUE)
  )
  
scored_shots_2.0 = shots_by_ft %>%
  left_join(league_avearage_2.0 %>%
              select(distance_ft, SHOT_ZONE_BASIC, SHOT_TYPE, expected_points_league),
            by = c("distance_ft", "SHOT_ZONE_BASIC", "SHOT_TYPE")) %>%
  mutate(expected_goal = expected_points_league)

Expected_vs_Actual_2.0 = scored_shots_2.0 %>%
  group_by(TEAM_NAME)%>%
  summarize( 
    expected_points = sum(expected_goal, na.rm = TRUE),
    actual_points = sum(Points_rule, naa.rm = TRUE),
    number_of_shots = n(),
    .groups = "drop"
  )%>%mutate(residual = actual_points - expected_points)

ggplot(Expected_vs_Actual_2.0,
       aes(x = reorder(TEAM_NAME, residual),
           y = residual,
           fill = residual > 0))+
  geom_col()+
  coord_flip()+
  labs(title = "Residuals(Actual - Expected) by Team",xlab = "Team", ylab = "Residual Points")


ggplot(Expected_vs_Actual_2.0,
       aes(x = reorder(TEAM_NAME, expected_points),
           y = expected_points,
           fill = expected_points))+
  geom_col()+
  coord_flip()+
  labs(title = "Total Expected Points by Team",
       x = "Team",
       y = "Expected Total Points")+
  theme_minimal()
```
### Now I wanted to apply Naive Binning and Averaging to the defensive side of the game. Instead of modeling how efficiently each team scores, my goal was to estimate how efficietnly each team defened: whether opponents score more or fewer points than expected given the quality of shots they take. I began by constructing a defesnive dataset from the shots_2025. I added variables that identifies which team was the opponent on each play. HTM and VTM identify the home and visitng teams for each shot. I created the variable called Oppponents to assign the opposing team tthat defened the shot. This allows each shot to be analyzed from the defender's perspective. Like in offense, I removed any missing distances and created one foot distance. I then computed league avearge defensive expectations by shot distance and type. These league averages represent the expected number of points opponents should score from each spot if all teams defended equally. To evaluate how well each team defended relative to league, I joined the league averages back to the shot level. Each shot now has an expected_goal value that represents what an average defedner would have allowed from that spot on the court. I then aggregated these values by defending team: expected_points_allowed shows how many points a team would have allowed if defending at league average efficiency, actual_points_allowed shows how many points the team's opponents actually scored, and residuals_allowed shows the difference between actual and expected points.Positive value means team had a worse than expected defense and negative value means team had a better than expected defense. The first graph shows which defense held opponents above or below league avearge expectations. Teams that had positie residuals(blue) like Utah Jazz, Sacramento Kings, and Golden State Warriors indiciate they had lots of below average defesnive execution that caused opponents to overperform their expected shot value. While teams that had negative residuals(red) like Oklahoma City Thunder, Cleveland Cavaliers, and Chicago Bulls had lots of above average defensive execution that caused opponents to underperform thier expected shot value. The second graph shows each team's total expected points allowed. Teams like Chicago Bulls, San Antonio Spurs, and New Oreleans Pelicans had highest expected totals allowed suggesting that they allowed opponents to take lots of high quality shot attempts. Meanwhile teams like Orlando Magic, Golden State Warriors and Brooklyn Nets had lowest expected totals allowed suggesting that they forced opponents to take lots of low quality shot attempts. These plots provide a comprehensive view of defensive performance: Defensive execution efficiency and Shot quality prevention. 

```{r}
shots_defense = shots_2025 %>%
  select(GAME_ID, HTM, VTM, PLAYER_NAME, TEAM_NAME, SHOT_DISTANCE, SHOT_TYPE,
         SHOT_ZONE_BASIC, SHOT_MADE_FLAG, PTS)%>%
  mutate(
    SHOT_DISTANCE = as.numeric(SHOT_DISTANCE),
    Points_rule = case_when(
      SHOT_MADE_FLAG == 1 & SHOT_TYPE == "3PT Field Goal" ~ 3,
      SHOT_MADE_FLAG == 1 & SHOT_TYPE == "2PT Field Goal" ~ 2,
      TRUE ~ 0
         ),
    team_abbreviation = Team_abbreviation[TEAM_NAME],
    Opponents = case_when(
      team_abbreviation == HTM ~ VTM,
      team_abbreviation == VTM ~ HTM,
      TRUE ~ NA_character_
    ))

shots_by_ft_defense = shots_defense%>%
  filter(!is.na(SHOT_DISTANCE))%>%
  mutate(distance_ft = floor(SHOT_DISTANCE))

league_average_defense = shots_by_ft_defense %>%
  group_by(distance_ft, SHOT_TYPE)%>%
  summarize(
    attempt_league = n(),
    made_shots_league = sum(SHOT_MADE_FLAG == 1, na.rm = TRUE),
    fg_percentage_league = attempt_league / made_shots_league, 
    expected_points_league = mean(Points_rule, na.rm = TRUE)
  )

scored_shots_defense = shots_by_ft_defense %>%
  left_join(league_average_defense %>%
              select(distance_ft, SHOT_TYPE, expected_points_league),
            by = c("distance_ft","SHOT_TYPE")) %>%
  mutate(expected_goal = expected_points_league)

Expected_vs_Actual_defense = scored_shots_defense %>%
  group_by(Opponents)%>%
  summarize(
    expected_points_allowed = sum(expected_goal, na.rm = TRUE),
    actual_points_allowed = sum(Points_rule, naa.rm = TRUE),
    number_of_shots_faced = n(),
    .groups = "drop"
  )%>% mutate(residuals_allowed = actual_points_allowed - expected_points_allowed )


ggplot(Expected_vs_Actual_defense,
       aes(x = reorder(Opponents, residuals_allowed),
           y = residuals_allowed,
           fill = residuals_allowed > 0))+
  geom_col()+
  coord_flip()+
  labs(title = "Residuals allowed(Actual - Expected) by Team",xlab = "Team", ylab = "Residual Points Allowed")


ggplot(Expected_vs_Actual_defense,
       aes(x = reorder(Opponents, expected_points_allowed),
           y = expected_points_allowed,
           fill = expected_points_allowed))+
  geom_col()+
  coord_flip()+
  labs(title = "Total Expected Points allowed by Team",
       x = "Team",
       y = "Expected Total Points Allowed")+
  theme_minimal()

```
### Now I decided to merge offesnive and defensive metrics of teams to evaluate overall efficiency. By combining each team's expected and actual points scored with the corresponding expected and actual points allowed, we can obtain a measure of net performance realtive to league average shot quality. First, I decided to standarized team abbrevation so I can align the offense (Expected_vs_Actual_2.0) with defense (Expected_vs_Actual_defense) so there is no problem in the merge. Then I decided to merge offense and defense. expected_combined_points shows a expected points scored minus expected points allowed, actual_combined_points shows an actual points scored - actual points allowed, and residual_combined shows the difference(actual - expected) in net performance. Positive residual indiciates that a team outperformed its combined expectation, while negative residuals indicates that a team underperformed its combined expectation relative to league average shot quality. The first graph, shows the teams ranked by net residual points, summarzing whether each team's overall performance exceeded or fell below expectations. The teams with positive residuals(blue) are Oklahoma City Thunder, Cleveland Cavaliers, and Boston Celtics suggesting these teams have done greate job capitalizing shot opportuntities and protected shot opportuntities of opponents compared with league average expectation. The teams with negative residuals(red) are Charlotte Hornets, Washington Wizards, Portland Trailblazers suggesting theset teams have done poor job capitalzing shot opportunities or protected agaisnt shot opportunities compared with league average expectation. The second graph, shows expected combined points (expected points scored - expected points allowed). The teams with positive residuals(blue) like Oklahoma City Thunder, Houston Rockets, and Atlanta Hawks suggest that they generated efficient shots while forcing oppoentns to shoot inefficient shots. Meanwhile the teams with negative residuals(red) like Charlotte Hornets, Philadelphia 76ers, and New Orelans Pelicans suggest that the generated inefficient shots while allowing opponents to shoot efficient shots. 

```{r}
Expected_vs_Actual_2.0 = Expected_vs_Actual_2.0 %>%
  mutate(Team_abbreviation = Team_abbreviation[TEAM_NAME])


Offense_and_Defense = Expected_vs_Actual_2.0 %>%
  select(TEAM_NAME, Team_abbreviation,
         expected_points_scored = expected_points,
         actual_points_scored = actual_points,
         resiudal_scored = residual)%>%
  left_join(
    Expected_vs_Actual_defense%>%
      select(Opponents, 
             expected_points_allowed,
             actual_points_allowed,
             residuals_allowed),
    by = c("Team_abbreviation" = "Opponents")) %>%
      mutate(
        expected_combined_points = expected_points_scored - expected_points_allowed,
        actual_combined_points = actual_points_scored - actual_points_allowed,
        residual_combined = resiudal_scored - residuals_allowed
      )
  
ggplot(Offense_and_Defense, 
       aes(x = reorder(TEAM_NAME, residual_combined),
           y = residual_combined, 
           fill = residual_combined > 0))+
  geom_col()+
  coord_flip()+
  labs(title = "Points Differences per 100 possesion residual",
       x = "Team",
       y = "Net Residual Points")+
  theme_minimal()

ggplot(Offense_and_Defense, 
       aes(x = reorder(TEAM_NAME, expected_combined_points),
           y =  expected_combined_points,
           fill = expected_combined_points))+
  geom_col()+
  coord_flip()+
  labs(title = "Total Expected Points difference by Teams",
       x = "Team",
       y = "Expected Combined points")
```
### The purpose of this is to estimate each team's expected win percentage using the Pythagorean win model, which is based on the fundamental idea that teams who score more points and allow fewer points should win more games  on average. I decided to do this because I believed this is a way to transition from expected scroing efficiency I was able to get from Naive Binning into season performance expectation. The first two lines extract team level totals from expected points model. Expected_Offesne shows how many points each team was expected to score based on its shooting efficiency and Expected_Defense shows how many points each team was expected to allow based on opponent shooting outcomes. I knew that All NBA teams play 82 games but still decided to count the total number of games each team played in Games_per_team, in case of missing data. Then I calculated Pythagorean Win percentage by first joining the datas together and convert to per-game metrics where expected_points showed a expected points scored per game and expected_points_agaisnt showed expected points allowed per game. Then I mannualy wrote down Pythagoren formula and estimated season record. I rounded the numbers to be whole because actual NBA records are shown as whole number. 

```{r}

Expected_Offense = Expected_vs_Actual_2.0 %>% 
  mutate(Team_abbreviation = Team_abbreviation[TEAM_NAME])%>% 
  select(TEAM_NAME, Team_abbreviation, expected_points_scored = expected_points) 

Expected_Defense = Expected_vs_Actual_defense %>% 
  rename(Team_abbreviation = Opponents)%>% select(Team_abbreviation, expected_points_allowed) 

Games_per_team = shots_2025 %>% 
  select(TEAM_NAME, GAME_ID)%>% 
  distinct()%>% 
  count(TEAM_NAME, name = "games") 

pythagorean_win = Expected_Offense %>% 
  left_join(Expected_Defense, by = "Team_abbreviation")%>% 
  left_join(Games_per_team, by = "TEAM_NAME")%>% 
  mutate(expected_points = expected_points_scored / games, expected_points_against = expected_points_allowed / games, pythagorean_win_percentage = (expected_points^1.83) / ((expected_points^1.83)+(expected_points_against^1.83)), expected_wins = pythagorean_win_percentage * games, expected_losses = games - expected_wins)

pythagorean_win = pythagorean_win %>%
  mutate(expected_wins = round(expected_wins),
         expected_losses = round(expected_losses))
```
### Now to test how well the model's Pythagorean Win line up with actual win percentage from 2025 NBA season, I decided to evalute it using Brier Score and Log Loss. First I wrote down the formulas used to get the number for Brier score and Log Loss, then I joined predicitons to truth by creating true_win_percentage(from actual standings) and predicited_win_percentage(from Pythagorean Win). Then I computed Brier Score and Log Loss. Brier score was 0.0197 and Log Loss was 0.6812. 

```{r}
nba_standings = tribble(
  ~TEAM_NAME, ~W, ~L, ~WinPct,
  "CLE", 64, 18, 0.780,
  "BOS", 61, 21, 0.744,
  "NYK", 51, 31, 0.622,
  "IND", 50, 32, 0.610,
  "MIL", 48, 34, 0.585,
  "DET", 44, 38, 0.537,
  "ORL", 41, 41, 0.500,
  "ATL", 40, 42, 0.488,
  "CHI", 39, 43, 0.476,
  "MIA", 37, 45, 0.451,
  "TOR", 30, 52, 0.366,
  "BKN", 26, 56, 0.317,
  "PHI", 24, 58, 0.293,
  "CHA", 19, 63, 0.232,
  "WAS", 18, 64, 0.220,
  "OKC", 68, 14, 0.829,
  "HOU", 52, 30, 0.634,
  "LAL", 50, 32, 0.610,
  "DEN", 50, 32, 0.610,
  "LAC", 50, 32, 0.610,
  "MIN", 49, 33, 0.598,
  "GSW", 48, 34, 0.585,
  "MEM", 48, 34, 0.585,
  "SAC", 40, 42, 0.488,
  "DAL", 39, 43, 0.476,
  "PHX", 36, 46, 0.439,
  "POR", 36, 46, 0.439,
  "SAS", 34, 48, 0.415,
  "NOP", 21, 61, 0.256,
  "UTA", 17, 65, 0.207
)

brier = function(y, phat){
  return(mean((y-phat)^2))
}

log_loss = function(y, phat){
  if(any(phat < 1e-12)) phat[phat < 1e-12] <- 1e-12
  if(any(phat > 1-1e-12)) phat[phat > 1-1e-12] <- 1-1e-12
  return(-1 * mean( y * log(phat) + (1-y) * log(1-phat)))
}

Comparison = pythagorean_win %>%
  select(TEAM_NAME, Team_abbreviation, pythagorean_win_percentage) %>%
  left_join(nba_standings, by = c("Team_abbreviation" = "TEAM_NAME"))%>%
  rename(true_win_percentage = WinPct, predicted_win_percentage = pythagorean_win_percentage)

brier_comparison = brier(Comparison$true_win_percentage, Comparison$predicted_win_percentage)
brier_comparison

log_loss_season = log_loss(Comparison$true_win_percentage, Comparison$predicted_win_percentage)
log_loss_season

```
### I decided to simulate game predicitons from the team level win probabilities estimated earlier with the Pythagorean Model. I wanted to conver the team level probabilities into several of simulated individual games, then measure how accruate model's implied probabilities would be if the season were replayed many times. First placed a random seed so random draws are replicable and I would get same sequence for my outcomes. Then I had to define my simulation function. uncount(games_per_team) is done to expand the data so each team has one simulated record per game. actual was done by rbinom to randomly simulate wins and losses according to each team's true win probability from the real standing. prediciton was done to predict win probabilties between 1e-12 and 1- 1e-12 to avoid errors. each_brier and each_log_loss computes the Brier and Log loss errors for every simulated game using the predicited probabilties. Then it returns the one mean Brier and mean Log loss score for that simulated run. Now to make it better I decided to repeat the simulation three times and bind the results together to smooth out randomness. Finally I computed the average Brier and average log loss across all runs, giving one stable prediciotn quality. Mean Brier was 0.245 and Mean Log Loss was 0.683. The tings to improve on: no free throw, treats all players and teams as equal within each shot zone (ex consider Stephen Curry and Ben Simmons having same chance at three point line), lots of context like shot clock, pressure,game situation is ignroed, same play style(don't consider different pace teams can have)

```{r}
set.seed(5)

simulate_score = function(df, games_per_team = 82){
  game_level = df %>%
    tidyr::uncount(games_per_team) %>%
    mutate(actual = rbinom(n(), size = 1, prob = true_win_percentage),
           prediction = pmin(pmax(predicted_win_percentage, 1e-12),1-1e-12),
           each_brier = (prediction-actual)^2,
           each_log_loss = -(actual * log(prediction)+(1-actual)* log(1-prediction))
           )
  tibble(mean_brier = mean(game_level$each_brier),
         mean_log_loss = mean(game_level$each_log_loss))}

results = bind_rows(
  simulate_score(Comparison),
  simulate_score(Comparison),
  simulate_score(Comparison))

results %>% summarize(average_brier = mean(mean_brier),
                      average_log_loss = mean(mean_log_loss))

```





